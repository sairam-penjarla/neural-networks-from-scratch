{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from graphviz import Digraph\n",
    "\n",
    "def trace(root):\n",
    "  # builds a set of all nodes and edges in a graph\n",
    "  nodes, edges = set(), set()\n",
    "  def build(v):\n",
    "    if v not in nodes:\n",
    "      nodes.add(v)\n",
    "      for child in v.parents:\n",
    "        edges.add((child, v))\n",
    "        build(child)\n",
    "  build(root)\n",
    "  return nodes, edges\n",
    "\n",
    "def draw_dot(root):\n",
    "  dot = Digraph(format='svg', graph_attr={'rankdir': 'LR'}) # LR = left to right\n",
    "  \n",
    "  nodes, edges = trace(root)\n",
    "  for n in nodes:\n",
    "    uid = str(id(n))\n",
    "    # for any neuron in the graph, create a rectangular ('record') node for it\n",
    "    dot.node(name = uid, label = f\" {n.label} | weight: {n.weight} | grad: {n.grad}\", shape='record')\n",
    "    if n._op:\n",
    "      # if this neuron is a result of some operation, create an op node for it\n",
    "      dot.node(name = uid + n._op, label = n._op)\n",
    "      # and connect this node to it\n",
    "      dot.edge(uid + n._op, uid)\n",
    "\n",
    "  for n1, n2 in edges:\n",
    "    # connect n1 to the op node of n2\n",
    "    dot.edge(str(id(n1)), str(id(n2)) + n2._op)\n",
    "\n",
    "  return dot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    y = 2*x + 8*x - x\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = 4\n",
    "y = f(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dy/dx is the derivative \n",
    "\n",
    "# how does x effect my y?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(a,b,c):\n",
    "    return 2*a + 8*b - c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 3.0\n",
    "b = 4.0\n",
    "c = 5.0\n",
    "h = 0.1\n",
    "\n",
    "(f(a,b,c+h) - f(a,b,c))/h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dy/da = 2.0000000000000284\n",
    "\n",
    "# dy/db = 7.999999999999972\n",
    "\n",
    "# dy/dc = -1.0000000000000142"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class neuron:\n",
    "  \n",
    "    def __init__(self, weight, parents=[], symbol='', label=''):\n",
    "        self.weight = weight\n",
    "        self.grad = 0.0\n",
    "        self.parents = parents\n",
    "        self._op = symbol\n",
    "        self.label = label\n",
    "        self.change_grad = self.blank\n",
    "        \n",
    "    def blank(self):\n",
    "        return None\n",
    "    def __repr__(self):\n",
    "        return f\"neuron(data={self.weight})\"\n",
    "    \n",
    "    def __add__(self, other):\n",
    "        other = other if isinstance(other, neuron) else neuron(other)\n",
    "        out = neuron(self.weight + other.weight, parents = [self, other], symbol = '+')\n",
    "        \n",
    "        def change_grads():\n",
    "            self.grad += 1.0 * out.grad\n",
    "            other.grad += 1.0 * out.grad\n",
    "        out.change_grad =  change_grads\n",
    "        \n",
    "        return out\n",
    "\n",
    "    def __mul__(self, other):\n",
    "        other = other if isinstance(other, neuron) else neuron(other)\n",
    "        out = neuron(self.weight * other.weight, parents = [self, other], symbol = '*')\n",
    "        \n",
    "        def change_grads():\n",
    "            self.grad += other.weight * out.grad\n",
    "            other.grad += self.weight * out.grad\n",
    "        out.change_grad =  change_grads\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def __pow__(self, other):\n",
    "        assert isinstance(other, (int, float)), \"only supporting int/float powers for now\"\n",
    "        out = neuron(self.weight**other, parents = [self], symbol = '**{other}')\n",
    "\n",
    "        def change_grads():\n",
    "            self.grad += other * (self.weight ** (other - 1)) * out.grad\n",
    "        out.change_grad =  change_grads\n",
    "\n",
    "        return out\n",
    "    \n",
    "    def __rmul__(self, other): # other * self\n",
    "        return self * other\n",
    "\n",
    "    def __truediv__(self, other): # self / other\n",
    "        return self * other**-1\n",
    "\n",
    "    def __neg__(self): # -self\n",
    "        return self * -1\n",
    "\n",
    "    def __sub__(self, other): # self - other\n",
    "        return self + (-other)\n",
    "\n",
    "    def __radd__(self, other): # other + self\n",
    "        return self + other\n",
    "\n",
    "    def tanh(self):\n",
    "        x = self.weight\n",
    "        t = (math.exp(2*x) - 1)/(math.exp(2*x) + 1)\n",
    "        out = neuron(t, parents = [self], symbol = 'tanh')\n",
    "        \n",
    "        def change_grads():\n",
    "            self.grad += (1 - t**2) * out.grad\n",
    "        out.change_grad = change_grads\n",
    "        \n",
    "        return out\n",
    "    def backprop(self):\n",
    "        parents = []\n",
    "        def print_parents(child):\n",
    "            if child.parents:\n",
    "                for parent in child.parents:\n",
    "                    print_parents(parent)\n",
    "            parents.append(child)\n",
    "\n",
    "        self.grad = 1\n",
    "        print_parents(self)\n",
    "\n",
    "        for i in reversed(parents):\n",
    "            i.change_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = neuron(0.2) ; a.label = \"A\"\n",
    "b = neuron(0.3) ; b.label = \"B\"\n",
    "c = neuron(0.4) ; c.label = \"C\"\n",
    "f = neuron(2.0) ; f.label = \"F\"\n",
    "\n",
    "\n",
    "\n",
    "d = a * b ; d.label = \"D\"\n",
    "e = d + c ; e.label = \"E\"\n",
    "g = e * f; g.label = \"G\"\n",
    "h = g.tanh(); h.label = \"h\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_dot(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h.backprop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inputs x1,x2\n",
    "x1 = neuron(2.0, label='x1')\n",
    "x2 = neuron(0.0, label='x2')\n",
    "\n",
    "\n",
    "# weights w1,w2\n",
    "w1 = neuron(-3.0, label='w1')\n",
    "w2 = neuron(1.0, label='w2')\n",
    "\n",
    "# bias of the neuron\n",
    "b = neuron(6.8813735870195432, label='b')\n",
    "\n",
    "\n",
    "# x1*w1 + x2*w2 + b\n",
    "x1w1 = x1*w1; x1w1.label = 'x1*w1'\n",
    "x2w2 = x2*w2; x2w2.label = 'x2*w2'\n",
    "\n",
    "x1w1x2w2 = x1w1 + x2w2; x1w1x2w2.label = 'x1*w1 + x2*w2'\n",
    "\n",
    "n = x1w1x2w2 + b; n.label = 'n'\n",
    "\n",
    "o = n.tanh(); o.label = 'o'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "o.backprop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_dot(o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class perceptron:\n",
    "    def __init__(self, num_of_weights):\n",
    "        self.weights = [neuron(random.uniform(-1,1)) for x in range(num_of_weights)]\n",
    "        self.bias = neuron(random.uniform(-1,1))\n",
    "\n",
    "    def __call__(self, input):\n",
    "        wixi = []\n",
    "        for w,x in zip(self.weights, input):\n",
    "            wixi.append(w*x)\n",
    "        result = sum(wixi) + self.bias\n",
    "        self.output = result.tanh()\n",
    "        return self.output\n",
    "    \n",
    "    def get_weights(self):\n",
    "        return self.weights + [self.bias]\n",
    "\n",
    "class layer:\n",
    "    def __init__(self, num_of_weights, num_of_perceptrons):\n",
    "        self.layer = [ perceptron(num_of_weights) for i in range(num_of_perceptrons) ]\n",
    "\n",
    "    def __call__(self, input):\n",
    "        self.perceptrons = []\n",
    "        for p in self.layer:\n",
    "            output = p(input)\n",
    "            self.perceptrons.append(output)\n",
    "        return self.perceptrons\n",
    "    \n",
    "    def get_weights(self):\n",
    "        layer_weights = []\n",
    "        for p in self.layer:\n",
    "            p_weights = p.get_weights()\n",
    "            layer_weights.extend(p_weights)\n",
    "        return layer_weights\n",
    "\n",
    "\n",
    "class mlp:\n",
    "    def __init__(self, num_input_neurons, n_layers):\n",
    "        self.layers = []\n",
    "\n",
    "        sz = [num_input_neurons] + n_layers\n",
    "        self.layers_list = [(sz[i], sz[i+1]) for i in range(len(n_layers))]\n",
    "        for num_of_weights, num_of_perceptrons in self.layers_list:\n",
    "            self.layers.append(layer(num_of_weights, num_of_perceptrons))\n",
    "\n",
    "    def train(self, x):\n",
    "        for l in self.layers:\n",
    "            x = l(x)\n",
    "        return x[0] if len(x) == 1 else x\n",
    "    \n",
    "    def get_weights(self):\n",
    "        all_weights = []\n",
    "        for l in self.layers:\n",
    "            all_weights.extend(l.get_weights())\n",
    "        return all_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data = [\n",
    "  [2.0, 3.0, -1.0],\n",
    "  [3.0, -1.0, 0.5],\n",
    "  [0.5, 1.0, 1.0],\n",
    "  [1.0, 1.0, -1.0],\n",
    "]\n",
    "y_test = [1.0, -1.0, -1.0, 1.0] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = mlp(len(x_data),[4,4,4,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# draw_dot(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 0, Loss = 5.938402125270562\n",
      "epoch = 1, Loss = 4.203418796681298\n",
      "epoch = 2, Loss = 4.281938582368888\n",
      "epoch = 3, Loss = 4.235674698489852\n",
      "epoch = 4, Loss = 4.362801444242429\n",
      "epoch = 5, Loss = 4.60102939828124\n",
      "epoch = 6, Loss = 4.36697130427215\n",
      "epoch = 7, Loss = 5.208261174141072\n",
      "epoch = 8, Loss = 3.7479178686828662\n",
      "epoch = 9, Loss = 3.5928629144417936\n",
      "epoch = 10, Loss = 4.3078791114869865\n",
      "epoch = 11, Loss = 3.5955932353037583\n",
      "epoch = 12, Loss = 3.774535733200117\n",
      "epoch = 13, Loss = 3.559820971563397\n",
      "epoch = 14, Loss = 4.037041506331686\n",
      "epoch = 15, Loss = 3.900129828604707\n",
      "epoch = 16, Loss = 3.74764675691695\n",
      "epoch = 17, Loss = 3.55552347353598\n",
      "epoch = 18, Loss = 4.106915674358619\n",
      "epoch = 19, Loss = 3.791529182691841\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(20):\n",
    "    # forward propogation\n",
    "    y_pred = []\n",
    "    for x in x_data:\n",
    "        y_pred.append( model.train(x))\n",
    "    loss = sum([(yout - ygt)**2 for ygt, yout in zip(y_test, y_pred)])\n",
    "\n",
    "    for w in model.get_weights():\n",
    "        w.grad = 0\n",
    "    # backward propagation\n",
    "    loss.backprop()\n",
    "    for w in model.get_weights():\n",
    "        w.weight += -0.0001 * w.grad\n",
    "\n",
    "    print(f\"epoch = {epoch}, Loss = {loss.weight}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.mlp at 0x7fceb400e770>"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
